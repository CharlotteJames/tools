{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we show how Twitter and clickstream data can be processed to extract articles from urls, and how we extract topics from news articles using LDA. \n",
    "We cover the following steps: \n",
    "\n",
    "1. Extracting links from tweets. \n",
    "2. Expanding URLs.\n",
    "3. Restricting to news domains and articles.\n",
    "4. Extracting text from articles. \n",
    "5. Restricting articles to those that mention Brexit. \n",
    "6. Topic models on Brexit articles. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting links from tweets\n",
    "\n",
    "Data privacy and Twitter terms of service do not allow us to share original Clickstream and Twitter data. We do, however, show how the data has been processed to obtain a list of urls from each source. \n",
    "\n",
    "We store most of our data in MongoDB, a non-relational database which is more convenient and efficient for storing tweets, articles as well as other types of documents with a rich structure. The script below assumes that the data is stored in a MongoDB collection, but it can be easily modified to deal with raw json data, and we show later an example of working with a json file. The package we are using for woking with MongoDB in Python is Pymongo. \n",
    "Our first task is to select only tweets written between 17 February and 23 June 2016 and pull out the urls, along with other useful information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n"
     ]
    }
   ],
   "source": [
    "import pymongo \n",
    "\n",
    "#Check if we can connect to the database:\n",
    "try:\n",
    "    connection=pymongo.MongoClient()\n",
    "    print \"Connected successfully!!!\"\n",
    "except pymongo.errors.ConnectionFailure, e:\n",
    "   print \"Could not connect to MongoDB: %s\" % e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74126534"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Connect to the database and select the \n",
    "#conn = pymongo.MongoClient('localhost', 27017)\n",
    "db = connection['HWtwitter']\n",
    "collection = db['brexit']\n",
    "\n",
    "#How many tweets do we have in this collection?\n",
    "collection.find().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import datetime modules for working with dates\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "# Define date range of interest\n",
    "start_date = datetime(2016, 2, 17, 00, 00, 0, 0, pytz.UTC)\n",
    "end_date = datetime(2016, 6, 24, 00, 00, 0, 0, pytz.UTC)\n",
    "# Check if range defined correctly. Is end date after start date? \n",
    "end_date>start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open a csv file and write out the relevant fields for the tweets that \n",
    "# are in the date range, and which include urls. \n",
    "import csv\n",
    "import unicodedata\n",
    "#Open a nex csv file to write to it: \n",
    "with open('brexit_links.csv', 'w') as outfile:                               \n",
    "    writer = csv.writer(outfile, delimiter='\\t', lineterminator='\\n')\n",
    "    header = ['mongo_id',  'tweet_id', 'user_id', 'date_str', \\\n",
    "              'number_retweets', 'user_followers', 'is_retweet', \\\n",
    "              'orig_user_id', 'orig_number_retweets', \\\n",
    "              'orig_user_followers', 'expanded_url_1']    \n",
    "    #write the header with the names of the \\\n",
    "    # variables of interest to file:\n",
    "    writer.writerow(header)                                                                            \n",
    "    #use the very useful Cursor method to loop through \\\n",
    "    #the tweets in the database. Define the cursor. \n",
    "    cursor=collection.find()     \n",
    "    for i in cursor:\n",
    "        #Turn tweet date field to utf-8 format \\\n",
    "        #to prevent character encoding errors.  \n",
    "        utf8_date=unicodedata.normalize('NFKD', \\\n",
    "                                        i[\"created_at\"]).encode('utf-8')                       \n",
    "        # Turn date field into datetime format. \n",
    "        tweet_date=datetime.strptime(utf8_date,\\\n",
    "                                     '%a %b %d %H:%M:%S +0000 %Y')\n",
    "        #If tweet ascreated between the dates of interest, \n",
    "        #extract the following variables of interest: \n",
    "        if (pytz.utc.localize(tweet_date)>=start_date and \\\n",
    "            pytz.utc.localize(tweet_date)<=end_date):    \n",
    "            try:\n",
    "                # Only select tweets that include urls. \n",
    "                # This works because all the other fields \n",
    "                # defined at this level are always in the data.\n",
    "                # So this only fails if there is no url. \n",
    "                expanded_url_1=i[\"entities\"][\"urls\"][0][\"expanded_url\"]\n",
    "                mongo_id=i[\"_id\"]\n",
    "                tweet_id=i[\"id\"]\n",
    "                user_id=i[\"user\"][\"id_str\"]\n",
    "                date_str=tweet_date\n",
    "                number_retweets=i[\"retweet_count\"]\n",
    "                user_followers=i[\"user\"][\"followers_count\"]\n",
    "                try:\n",
    "                    # These fields only exist if the tweet is a retweet.\n",
    "                    # Otherwise, write them out as empty. \n",
    "                    retweet_id=i[\"retweeted_status\"][\"id\"]\n",
    "                    orig_user_id=i[\"retweeted_status\"][\"user\"][\"id_str\"]\n",
    "                    orig_number_retweets=i[\"retweeted_status\"][\"retweet_count\"]\n",
    "                    orig_user_followers=i[\"retweeted_status\"][\"user\"][\"followers_count\"]\n",
    "                    is_retweet=1\n",
    "                except: \n",
    "                    is_retweet=0\n",
    "                    retweet_id=0\n",
    "                    orig_number_retweets=0\n",
    "                    orig_user_followers=0\n",
    "                # Write results to csv: \n",
    "                writer.writerow([mongo_id]+[tweet_id]+[user_id]+[date_str]+\\\n",
    "                                [number_retweets]+[user_followers]+[is_retweet]+\\\n",
    "                                [orig_user_id]+[orig_number_retweets]+\\\n",
    "                                [orig_user_followers]+[expanded_url_1]])\n",
    "            except:     \n",
    "                pass\n",
    "\n",
    "# The output is a csv file with urls and information about the \n",
    "# tweets they were extracted from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read file into Pandas, remove duplicates in termd of urls, write out urls only. \n",
    "import pandas as pd\n",
    "# Read in data:\n",
    "df = pd.read_csv(\"brexit_links.csv\", sep=\"\\t\")\n",
    "# Drop duplicate values:\n",
    "df_no_duplic=df.drop_duplicates(subset='expanded_url_1',\\\n",
    "                                keep=\"first\")\n",
    "# Write new data to file. \n",
    "df_no_duplic[\"expanded_url_1\"].to_csv(\"brexit_unique_time_intervala.csv\", \\\n",
    "                                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving urls\n",
    "The same url can appear in the Twitter and Clickstream data under multiple forms, depending on whether it was shortened and the method used to shorten it, or if it is a redirect from social media, etc. By resolving the url we ensure that all these different formats are reduced to a single article url. This step is necessar since we don't want to waste any resources extracting text from the same article multiple times (and then merging them back together by text). \n",
    "The script below is used to resolve urls both in the Twitter data and in the Clickstream data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import csv\n",
    "import grequests\n",
    "import requests\n",
    "import re\n",
    "import itertools\n",
    "import sys\n",
    "import os\n",
    "import urlclean\n",
    "\n",
    "# Define working directory and file names: \n",
    "os.chdir(\"F:/exponet/twitter_brexit\")\n",
    "F_NAME_IN = 'brexit_unique_time_interval.csv'\n",
    "F_NAME_OUT = 'brexit_unique_time_interval_expanded.csv'\n",
    "F_NAME_OUT_ERR = 'brexit_unique_urls_expanded_exceptions.csv'\n",
    "\n",
    "# Handle all http formats: \n",
    "rx_http = re.compile('(?<=http)s(?=:\\/\\/)')\n",
    "\n",
    "# Read in urls \n",
    "with open(F_NAME_IN,'r') as f_in:\n",
    "\turl_list = [row.strip('\\n') for row in f_in]\n",
    "\n",
    "# Define how exceptions are handled: \n",
    "class ExceptionHandler:\n",
    "\tdef __init__(self):\n",
    "\t\tself.f_out = open(F_NAME_OUT_ERR,'wb')\n",
    "\t\tself.w = csv.writer(self.f_out, delimiter='\\t', \\\n",
    "                            quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\t\t\n",
    "\tdef __enter__(self):\n",
    "\t\treturn self\n",
    "\t\t\n",
    "\tdef __exit__(self, exc_type, exc_value, traceback):\n",
    "\t\tself.f_out.close()\n",
    "\t\n",
    "\tdef callback(self, request, exception):\n",
    "\t\ttry:\n",
    "\t\t\tself.w.writerow([request.url, repr(exception.message)])\n",
    "\t\texcept UnicodeError:\n",
    "\t\t\tprint 'Following string is not UTF-8:'\n",
    "\t\t\tprint repr(request.url), repr(exception.message)\n",
    "\n",
    "def exception_handler(request, exception):\n",
    "\tpass\n",
    "\n",
    "# Disable warnings (not a very good idea, make sure you \\\n",
    "# read about it before using this option)\n",
    "requests.packages.urllib3.disable_warnings()\n",
    "\n",
    "# Define the function that retries the urls from the request response \n",
    "def get_url(response):\n",
    "\tif response.history:\n",
    "\t\tseries = list(itertools.chain(response.history,[response]))\n",
    "\t\tfor i in range(len(series)):\n",
    "\t\t\tif series[i].status_code == 301 or \\\n",
    "            (i < len(series) - 1 and rx_http.sub('', series[i].url) \\\n",
    "             == rx_http.sub('', series[i+1].url)): pass\n",
    "\t\t\telse: break\n",
    "\t\tretval = [series[0].url, series[i].url if i > 0 else '', \\\n",
    "                  series[i].status_code]\n",
    "\telse: retval = [response.url, '', response.status_code]\n",
    "\tresponse.close()\n",
    "\treturn retval\n",
    "\n",
    "# Visit all urls in the list: \n",
    "reqs = (grequests.head(u, verify = False, timeout = 3) \\\n",
    "        for u in url_list)\n",
    "\n",
    "# Write resolved urls to file: \n",
    "with open(F_NAME_OUT,'wb') as f_out, ExceptionHandler() as eh:\n",
    "\tw = csv.writer(f_out, delimiter='\\t', quotechar='\"', \\\n",
    "                   quoting=csv.QUOTE_MINIMAL)\n",
    "\tfor res in grequests.imap(reqs, exception_handler = eh.callback, \\\n",
    "                              size = 500):\n",
    "\t\tw.writerow(get_url(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After resolving the urls, we need to drop dupplicates again to end up with a set of unique urls. We do this just as we did before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in data:\n",
    "df = pd.read_csv(\"brexit_unique_time_interval_expanded.csv\", sep=\"\\t\")\n",
    "# Drop duplicate values:\n",
    "df_no_duplic=df.drop_duplicates(subset='expanded_url_1', keep=\"first\")\n",
    "# Write new data to file. \n",
    "df_no_duplic[\"expanded_url_1\"].to_csv(\"brexit_unique_expanded.csv\", \\\n",
    "                                      index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricting to news domains & articles\n",
    "We next restrict our data to a list of 460 major news domains only, and/or to news articles on those domains. To identify articles on news domains we take advantage of the structure of news websites, which place articles in a specific location in a database. We can therefore write regular expressions that identify articles based on that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read in list of domains and pre-coded regular expressions for the articles: \n",
    "\n",
    "news_domains=[]\n",
    "article_regex=[]\n",
    "with open(\"alexa_news_domains_no_duplic.csv\", \"r\") as infile:\n",
    "    reader=csv.reader(infile, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        news_domains.append(str(row[0]).strip())\n",
    "        article_regex.append(str(row[1]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter links to articles. \n",
    "with open('brexit_news_links.csv', 'w') as outfile:    \n",
    "    writer = csv.writer(outfile, delimiter='\\t', \\\n",
    "                        lineterminator='\\n')\n",
    "    with open(\"brexit_unique_expanded.csv\", \"r\") as infile:\n",
    "        reader=csv.reader(infile, delimiter=\"\\t\")\n",
    "        for row in reader:\n",
    "            domain_list=[i for i in news_domains if i \\\n",
    "                         in str(row[0])]\n",
    "            try:\n",
    "                domain=domain_list[0]\n",
    "            except:\n",
    "                domain=\"\"\n",
    "            writer.writerow(row+[domain])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting content from article links\n",
    "We extracted the title, text, author, date, and other information from the article links using Diffbot, a content-extraction service. Diffbot is free for processing up to 10.000 articles per month. You need to sign up and imput your token for the script below to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import diffbot\n",
    "import csv\n",
    "import time\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "token=\"your_diffbot_token_here\"\n",
    "\n",
    "# You can get up to 1 link per second with the free version, \n",
    "# so we can run multiple identical scripts at the same time. \n",
    "# We do this by dividing the data into 5 sets, and running \n",
    "# the script on each set: \n",
    "RUN = int(sys.argv[1])\n",
    "N_RUNS = int(sys.argv[2])\n",
    "\n",
    "F_NAME_OUT = 'brexit_diffbot_links_1%02d.json'%RUN\n",
    "F_NAME_OUT_ERR = 'brexit_diffbot_errors_1%02d.json'%RUN\n",
    "\n",
    "# Read in the links\n",
    "all_links=[]\n",
    "with open(\"brexit_unique_expanded.csv\", \"r\") as infile:\n",
    "        reader=csv.reader(infile)\n",
    "        for row in reader:\n",
    "            all_links.append(row[0])\n",
    "\n",
    "links=all_links[RUN :: N_RUNS]\n",
    "\n",
    "# Write the articles out as .json, one per line. \n",
    "start = time.time()\n",
    "with open(F_NAME_OUT_ERR, 'wb') as errfile:\n",
    "    writerr=csv.writer(errfile)\n",
    "    with open(F_NAME_OUT, 'wb') as outfile:\n",
    "        for link in links:\n",
    "            try:\n",
    "                json.dump(diffbot.article(link, \\\n",
    "                            token=token,\\discussion=False, \\\n",
    "                            timeout=20000), outfile)\n",
    "                outfile.write('\\n')\n",
    "            except:\n",
    "                writerr.writerow(link)\n",
    "                pass\n",
    "print time.time()-start, 'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to process the .json file from Diffbot to retain only the link, title, and text.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "thepath=glob.glob(\"brexit_diffbot_links_*.json\")\n",
    "# Read all files in the folder: \n",
    "files=[]\n",
    "for i in thepath:\n",
    "    base=os.path.basename(i)\n",
    "    files.append(base)\n",
    "\n",
    "# Take out the fields of interest from the json files,\n",
    "# write them out to a new file. \n",
    "with open ('brexit_url_content.csv', 'w') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter='\\t', lineterminator='\\n')\n",
    "    header=[\"link\", \"title\", \"text\"]\n",
    "    writer.writerow(header)\n",
    "    for filename in files:\n",
    "        with open(filename, 'r') as f: \n",
    "            for row in f:\n",
    "                try:\n",
    "                    diffbot_res = json.loads(row)\n",
    "                    language=diffbot_res['objects'][0]['humanLanguage']\n",
    "                    if language==\"en\":\n",
    "                        raw_title=diffbot_res['objects'][0]['title']\n",
    "                        title=\" \".join(raw_title.split())\n",
    "                        title=title.decode('utf-8', errors='ignore')\n",
    "                        raw_text=diffbot_res['objects'][0]['text']\n",
    "                        link=diffbot_res['request']['pageUrl']\n",
    "                        if raw_text!=\"\":\n",
    "                            text=\" \".join(raw_text.split())\n",
    "                            text=text.decode('utf-8', errors='ignore')\n",
    "                            writer.writerow([link]+[title]+[text])\n",
    "                except:\n",
    "                    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping only articles that mention the Brexit referendum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Select only articles that include brexit or referendum in title of body.\n",
    "\n",
    "with open ('mention_brexit_eu_referendum.csv', 'w') as outfile:\n",
    "    writer = csv.writer(outfile, delimiter='\\t', lineterminator='\\n')\n",
    "    with open('brexit_url_content.csv', 'r') as infile:\n",
    "        reader=csv.reader(infile, delimiter='\\t')\n",
    "        reader.next()\n",
    "        for line in reader:\n",
    "            if (\"brexit\" in line[1].lower() or \"brexit\" in line[2].lower() \\\n",
    "                or (\"eu referendum\" in line[1].lower() or \\\n",
    "                    \"membership referendum\" in line[1].lower() \\\n",
    "                    or \"european referendum\" in line[1].lower()  \\\n",
    "                    or \"uk referendum\" in line[1].lower() or \\\n",
    "                    \"eu referendum\" in line[2].lower() \\\n",
    "                    or \"membership referendum\" in line[2].lower() \\\n",
    "                    or \"european referendum\" in line[2].lower()  \\\n",
    "                    or \"uk referendum\" in line[2].lower() )):\n",
    "                writer.writerow(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic models\n",
    "We use the Python package Gensim to extract topics from our corpus of Brexit articles using an LDA model. \n",
    "The first step involves cleaning and tokenizing our text data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import required packages:\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import csv\n",
    "csv.field_size_limit(40000000)\n",
    "stemmer = PorterStemmer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stop_words = set(stopwords.words('english')+get_stop_words('en'))\n",
    "\n",
    "# Turn to lower case, stem, remove stopwords and numbers. \n",
    "texts=[]\n",
    "with open(\"mention_brexit_eu_referendum.csv\", \"r\") as infile:\n",
    "    reader=csv.reader(infile, delimiter=\"\\t\")\n",
    "    for row in reader:\n",
    "        text=row[3].decode('utf-8', 'ignore')\n",
    "        tokens=[word.lower() for word in \\\n",
    "                tokenizer.tokenize(stemmer.stem(text).lower()) if \\\n",
    "                ((word not in stop_words) and word.isalpha())]\n",
    "        texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn tokenized documents into an id to term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# discard terms that appear in less than 0.05 of documents,\n",
    "# and those that appear in more than 0.75 of them. \n",
    "dictionary.filter_extremes(no_below=0.05, no_above=0.75)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print('Number of unique tokens: %d' % len(tw_dictionary))\n",
    "print('Number of documents: %d' % len(tw_corpus))\n",
    "\n",
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 50\n",
    "chunksize = 2000\n",
    "passes = 30\n",
    "iterations = 300\n",
    "eval_every = None  \n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model_1 = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n",
    "             alpha='auto', eta='auto', \\\n",
    "             iterations=iterations, num_topics=num_topics, \\\n",
    "             passes=passes, eval_every=eval_every)\n",
    "\n",
    "with open(\"tw_cs_combined_topics_01.txt\", \"a\") as outfile:\n",
    "    outfile.write(\"\\n50 topics\\n\")\n",
    "    outfile.write('\\n'.join('%s %s' % x for x in \\\n",
    "                            model_1.print_topics(num_topics=50, num_words=30)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produces a list of 50 topics and the most important words for each of them. \n",
    "\n",
    "We can order these topics by topic coherence: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_topics = model_1.top_topics(corpus, num_words=30)\n",
    "# Average topic coherence is the sum of topic coherences \n",
    "#of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "\n",
    "with open(\"tw_cs_combined_topics_01.txt\", \"a\") as outfile:\n",
    "    outfile.write(\"\\nTopic coherence\\n\")\n",
    "    outfile.write('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "    for i in top_topics: \n",
    "        outfile.write(str(i))\n",
    "        outfile.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the probability of each topic in a document, and average thhose out by type of data: Twitter or clickstream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_number=0\n",
    "header=[\"twitter\", \"article_number\", \"topic\", \"probability\"]\n",
    "with open(\"tw_cs_topic_probabilities_01.csv\", \"w\") as outfile:\n",
    "    writer=csv.writer(outfile,  delimiter='\\t', lineterminator='\\n')\n",
    "    writer.writerow(header)\n",
    "    for text in texts[0:29567]: \n",
    "        twitter=0\n",
    "        article_number=article_number+1\n",
    "        bow = dictionary.doc2bow(text)\n",
    "        topics=model_1_all.get_document_topics(bow, minimum_probability=0.000001 )\n",
    "        for i in topics:\n",
    "            topic=i[0]\n",
    "            probability=i[1]\n",
    "            writer.writerow([twitter]+[article_number]+[topic]+[probability])\n",
    "    for text in texts[29567:]: \n",
    "        twitter=1\n",
    "        article_number=article_number+1\n",
    "        bow = dictionary.doc2bow(text)\n",
    "        topics=model_1_all.get_document_topics(bow, minimum_probability=0.000001 )\n",
    "        for i in topics:\n",
    "            topic=i[0]\n",
    "            probability=i[1]\n",
    "            writer.writerow([twitter]+[article_number]+[topic]+[probability])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then read in this file, aggregate by data source (Twitter or clickstream) and plot the average topic probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cs</th>\n",
       "      <th>article_number</th>\n",
       "      <th>topic</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.046642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cs  article_number  topic  probability\n",
       "0   0               1      0     0.001232\n",
       "1   0               1      1     0.000543\n",
       "2   0               1      2     0.046642\n",
       "3   0               1      3     0.000719\n",
       "4   0               1      4     0.000986"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_topics=pd.read_csv(\"F:/exponet/clickstream/tw_cs_topic_probabilities_01.csv\",\\\n",
    "                      sep=\"\\t\")\n",
    "df_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th>cs</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.036489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.039861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.008339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>0.008857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         probability\n",
       "                mean\n",
       "topic cs            \n",
       "0     0     0.036489\n",
       "      1     0.039861\n",
       "1     0     0.008339\n",
       "      1     0.004901\n",
       "2     0     0.008857"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group and aggregate data: \n",
    "grouped=df_topics.groupby([\"topic\", \"cs\"])\n",
    "aggregated=grouped.agg({\"probability\":[\"mean\"]})\n",
    "# Display only first 5 averaged probabilities: \n",
    "aggregated[0:5]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
